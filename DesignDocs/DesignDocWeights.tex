\documentclass{article}

 \usepackage{url} 
\usepackage{amsthm,amsmath,amssymb,indentfirst,float}
\usepackage{verbatim}
\usepackage[sort,longnamesfirst]{natbib}
\newcommand{\pcite}[1]{\citeauthor{#1}'s \citeyearpar{#1}}
\newcommand{\ncite}[1]{\citeauthor{#1}, \citeyear{#1}}
\DeclareMathOperator{\logit}{logit}
    \DeclareMathOperator{\var}{Var}
   %  \DeclareMathOperator{\det}{det}
     \DeclareMathOperator{\diag}{diag}

\usepackage{geometry}
%\geometry{hmargin=1.025in,vmargin={1.25in,2.5in},nohead,footskip=0.5in} 
%\geometry{hmargin=1.025in,vmargin={1.25in,0.75in},nohead,footskip=0.5in} 
%\geometry{hmargin=2.5cm,vmargin={2.5cm,2.5cm},nohead,footskip=0.5in}

\renewcommand{\baselinestretch}{1.25}

\usepackage{amsbsy,amsmath,amsthm,amssymb,graphicx}

\setlength{\baselineskip}{0.3in} \setlength{\parskip}{.05in}


\newcommand{\cvgindist}{\overset{\text{d}}{\longrightarrow}}
\DeclareMathOperator{\PR}{Pr} 
\DeclareMathOperator{\cov}{Cov}


\newcommand{\sX}{{\mathsf X}}
\newcommand{\tQ}{\tilde Q}
\newcommand{\cU}{{\cal U}}
\newcommand{\cX}{{\cal X}}
\newcommand{\tbeta}{\tilde{\beta}}
\newcommand{\tlambda}{\tilde{\lambda}}
\newcommand{\txi}{\tilde{\xi}}




\title{Design Document for Relevance-Weighting: R Package glmm}

\author{Sydney Benson}

\begin{document}
\maketitle{}

\begin{abstract}
This design document will give an overview of the changes made to the R package \texttt{glmm} with respect to a relevance-weighted likelihood method. We use relevance-weighting to better reflect the real-world occurrence of more or less informative observations.
\end{abstract}

\section{Introduction}
This project is meant to enable the user of the \texttt{glmm} function in the \texttt{glmm} R package to include an optional relevance-weighting scheme. A common assumption of linear models is that each observation in a data set is equally informative and trustworthy; however, in real-world data sets, this is rarely the case. Thus, the optional relevance-weighting scheme will allow the user to place a heavier weight on the more informative and/or trustworthy observations in their data set so that those data points that are less informative affect the model to a lesser degree.

Additionally, a change to the \texttt{glmm} summary relating to the certainty of estimates is outlined at the end of the document.

\section{The Process}
First, the function will need to establish whether the user has supplied a proper weighting scheme. Next, the weighting scheme will need to be applied in the \texttt{el.C}, \texttt{elval.C} and \texttt{elGH.C} functions. After defining the weighting vector, the remainder of this section will illustrate how this weighting scheme will be applied in the previously mentioned functions. Lastly, the changes made to the importance sampling distribution are outlined.

\subsection{The Weighting Vector}
If a proper weighting scheme has been supplied, the function will utilize that weighting scheme; if  the weighting scheme supplied is improper, the user will receive an error; and if no weighting scheme is supplied, a weighting vector of all ones will be created. The weights must be in the form of a vector and the vector must be the same length as the response vector, $y$. All weights must be numeric and non-negative. There can be no missing weights. 

\subsection{The Relevance-Weighted Log Density}
Following Hu and Zidek (1997), the relevance-weighted likelihood is defined as

\begin{align}
\text{REWL}(\theta) = \prod_{i=1}^n f(y_i | \theta)^{\lambda_i}
\end{align}

\noindent where $n$ is the length of the response vector, $y$. Since we define $\log f_\theta(y|u_k)$ as $\sum_i y_i\eta_i - c(\eta_i)$, where $\eta = X\beta + Zu$, $\log f_\theta(y_i | u_k) = y_i\eta_i - c(\eta_i)$ and $f_\theta(y_i|u_k) = \exp \left(  y_i\eta_i - c(\eta_i) \right)$. So, the relevance-weighted log density becomes

\begin{align}
\text{RWLD}(\theta) &= \prod_{i=1}^n \left[ \exp \left(  y_i\eta_i - c(\eta_i) \right) \right] ^{\lambda_i}  \\
&= \prod_{i=1}^n \exp \left[ \lambda_i \left(  y_i\eta_i - c(\eta_i) \right) \right]
\end{align}

\noindent Then,

\begin{align}
\log \text{RWLD}(\theta) &= \log \prod_{i=1}^n \exp \left[ \lambda_i \left(  y_i\eta_i - c(\eta_i) \right) \right] \\
&= \sum_{i=1}^n \log \exp \left[ \lambda_i \left(  y_i\eta_i - c(\eta_i) \right) \right] \\
&= \sum_{i=1}^n \lambda_i \left(  y_i\eta_i - c(\eta_i) \right)
\end{align}

\subsection{The First Derivative}

Remember that the derivative of the log density of the data with respect to one component, $\eta_j$, is 

\begin{align}
\dfrac{\partial}{\partial \eta_j} \log f_\theta (y|u) = y_j - c'(\eta_j).
\end{align}

\noindent Consequently, 

\begin{align}
\dfrac{\partial}{\partial \eta_j} \log \text{RWLD}(\theta) = \lambda_j \left[ y_j - c'(\eta_j) \right].
\end{align}

\noindent and the derivative of the component $\eta_j$ with respect to one of the fixed effect predictors, $\beta_j$, is 

\begin{align}
\dfrac{\partial \eta_j}{\partial \beta_l} = X_{jl}
\end{align}

\noindent Then, we can use the chain rule to find that

\begin{align}
\dfrac{\partial}{\partial \beta_l} \log \text{RWLD}(\theta) &= \Lambda \ X \left[ y- c'(\eta) \right]
\end{align}

\noindent where $\Lambda$ is the $n$ x $n$ weight matrix made from $\lambda$, which is the $n$ x $1$ weights vector given by the user or made of all ones if no vector is given. 

\subsection{The Second Derivative}

Similar to the first derivative, we find that the second derivative of the relevance-weighted log density of the data is 

\begin{align}
\dfrac{\partial^2}{\partial \beta_l^2} \log \text{RWLD}(\theta) &= X' \left[- c''(\eta) \right] \Lambda \ X
\end{align}

\subsection{The Importance Sampling Distribution}

The original importance sampling distribution is 

\begin{align}
\tilde{f}(u) &= p_1  \grave{f}(u|0,D^*)+p_2  f(u \, | \, u^*, D^*)+p_3  f(u \, | \, u^*, (Z'  c''(X \beta^*+Zu^*) Z +(D^*)^{-1}   )^{-1})
\end{align}

\noindent and the portion that is concerned with weighting is $Z' c"(X \beta^* + Zu^*) Z$. We know that $\eta = X \beta + Zu$, so this is similar to $Z' c"(\eta) Z$ which looks similar to the second derivative of the cumulant. That similarity is used to incorporate the weighting scheme into the importance sampling distribution. 

The weighted version of the importance sampling distribution follows as

\begin{align}
\tilde{f}(u) &= p_1  \grave{f}(u|0,D^*)+p_2  f(u \, | \, u^*, D^*)+p_3  f(u \, | \, u^*, (Z'  c''(X \beta^*+Zu^*) \Lambda \ Z +(D^*)^{-1}   )^{-1}) \\
&= p_1 \exp( \log  \grave{f}(u|0,D^*) ))+p_2 \exp( \log(  f(u \, | \, u^*, D^*) ))+ \nonumber \\
&+ p_3 \exp( \log( f(u \, | \, u^*, (Z'  c''(X \beta^*+Zu^*) \Lambda \ Z +(D^*)^{-1}   )^{-1}) )).
\end{align}

\noindent Then, if we set

\begin{align}
\tilde{a}= \max \left\{ \log  \grave{f}(u|0,D^*) ), \log  f(u \, | \, u^*, D^*) ), \log f(u \, | \, u^*, (Z'  c''(X \beta^*+Zu^*) \Lambda \ Z +(D^*)^{-1}   )^{-1})  \right\}
\end{align}

Then, 

\begin{align}
 \tilde{f}(u) 
&= p_1 \exp\left( \log  \grave{f}(u|0,D^*)  -\tilde{a}\right)+p_2 \exp \left( \log  f(u \, | \, u^*, D^* )-\tilde{a} \right)+ \nonumber\\
&+ p_3 \exp\left[ \log f \left(u \, | \, u^*, (Z'  c''(X \beta^*+Zu^*) \Lambda \ Z +(D^*)^{-1}   )^{-1} \right)  -\tilde{a} \right].
\end{align}

\section{Summary Change}

The coefficient estimates in the summary should be changed in order to reflect the certainty known from the Monte Carlo standard error (\texttt{mcse}). This means that \texttt{glmm} must:

\begin{enumerate}
\item Detect the position of the first non-zero digit in the \texttt{mcse} estimates;
\item Round the coefficient estimates in the \texttt{glmm} summary to the number of digits detected in step 1.
\item Round the standard error estimated in the \texttt{glmm} summary to the number of digits detected in step 1.
\end{enumerate}

\end{document}
